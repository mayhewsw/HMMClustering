% Template for ICME-2010 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf_ICME,amsmath,epsfig,fancyhdr}
\setlength{\paperwidth}{215.9mm} \setlength{\hoffset}{-9.7mm}
\setlength{\oddsidemargin}{0mm} \setlength{\textwidth}{184.3mm}
\setlength{\columnsep}{6.3mm} \setlength{\marginparsep}{0mm}
\setlength{\marginparwidth}{0mm} \setlength{\paperheight}{279.4mm}
\setlength{\voffset}{-7.4mm} \setlength{\topmargin}{0mm}
\setlength{\headheight}{0mm} \setlength{\headsep}{0mm}
\setlength{\topskip}{0mm} \setlength{\textheight}{235.2mm}
\setlength{\footskip}{12.4mm} \setlength{\parindent}{1pc}

\usepackage{minted}

\ICMEfinalcopy % *** Uncomment this line for the final submission

\def\ICMEPaperID{****} % *** Enter the ICME Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifICMEfinal\pagestyle{empty}\fi


\begin{document}\sloppy

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}


% Title.
% ------
\title{Using N-gram similarity metrics for Relation Clustering}
%
% Single address.
% ---------------
\name{Stephen Mayhew, Nicholas Kamper}

\address{
\textit{mayhewsw@rose-hulman.edu}\\
\textit{kampernj@rose-hulman.edu}\\
Rose-Hulman Institute of Technology}


\maketitle
% insert page header and footer here for IEEE PDF Compliant
\thispagestyle{fancy} \fancyhead{} \lhead{}
\lfoot{\copyright2012 SWM, NJK} \cfoot{}
%\rfoot{ICME 2010}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}


%
%\begin{abstract}
%This paper has to do with relation extraction. This is probably best written %after the rest of the paper is done.
%\end{abstract}
%
%\begin{keywords}
%Natural Language Processing, Information Extraction, Relation Extraction, %Coreference Resolution
%\end{keywords}
%
\section{Introduction}
\label{sec:intro}

The overall goal of our project was to use Hidden Markov Models (HMM) to cluster relations. A relation is a triple of the form (object, relation, object), such as \textit{(george washington, was president of, the usa)}. Such relations are the output of relation extractors. These relation extractors process a massive amount of web text, and inevitably produce many different strings that refer to the same thing. The goal of this system is to cluster these extractions by their meaning. For example, one such cluster might be:

\fvset{frame=single}
\begin{minted}[fontsize=\footnotesize]{text}
(tony blair, is prime minister of, britain) 
(tony blair, is prime minister of, great britain) 
(tony blair, is the prime minister of, the UK)
\end{minted}
\fvset{frame=none}

We note that facts do not need to be true to be part of the system.



\section{Experiments}
We began by looking into ways that HMM's could be used to solve the problem. Our thought process went: we will need to compare every sentence against every other to check similarity. We can train an HMM on a given sentence, and then test every other sentence on that HMM. The resulting score would be compared to some threshold in order to decide if those two strings should be clustered. 

This approach had many problems. First of all, that is a fragile model (how to deal with skips etc.). Second, one sentence does not train very well. 

Another line of thought was, perhaps the hidden states are meanings, and the text is an output. This was a good thought, but there is no limit on the number of meanings, and no limit on the number of outputs. 


HMM for scoring (Multiple sequence alignment). Dead wall.
Next we researched using a technique called Multiple Sequence Alignment, which is used in biological research to classify protein sequences. This seemed promising to begin with, but we realized that the domain of the protein alphabet is much smaller (4 symbols), and this caused problems for us. 

Having failed these two applications, we were at a loss. It seemed that HMM's may not be the best tool for the job.

Our next exploration focused on n-gram models. It seemed reasonable to use n-gram models in a similar way as described for HMM's above: when comparing every pair of strings, we create an n-gram model using the first string, and test it with the second one. Unsurprisingly, this failed for many of the same reasons. The scores it returned were largely erratic and useless. That is, it would classify strings as ``microsoft windows'' as being more likely to match ``barack obama'' than ``president obama'' would be. We were using NLTK, which in turn used n-gram models with Katz backoff, and Lidstone smoothing \cite{nltk}. 

Since we also had 87-GB of Google n-gram data at hand, we considered how we could use it. However, we could not find any good way to use it.

Before we landed on using NLTK, we tried several different libraries, including OpenNLP, KYLM. These, however, were either only partially implemented, or only partially documented. We found them to be of little help.

Finally, we chanced on Python n-gram similarity library that uses n-grams to predict similarity between two strings. This library is given the imaginative name of ``NGram". The documentation describes it as follows: ``The NGram class is a set that supports searching for its members by N-Gram string similarity." \cite{py_ngram_lib}

What is n-gram string similarity?

How did we use it?

What kind of results did we get?

What are we going to try to get better results?


\begin{thebibliography}{1}

\bibitem{py_ngram_lib}
Michel Albert, Graham Poulter, and open source contributors. Python NGram Similarity Library. \textit{http://packages.python.org/ngram/}

\bibitem{yates2009}
Alexander Yates, Oren Etzioni. \textit{Unsupervised Methods for Determining Object and Relation Synonyms on the Web}. Journal of Artificial Intelligence Research (JAIR). March, 2009.

\bibitem{nltk}
Bird, Steven, Edward Loper and Ewan Klein (2009). \textit{Natural Language Processing with Python}.  O'Reilly Media Inc.

\end{thebibliography}


\end{document}